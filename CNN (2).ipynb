{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uNds6sbE1Yp",
        "colab_type": "text"
      },
      "source": [
        "**CNN** - Daniel Conrado Monroy Madrid 16012674\n",
        "*Proyecto final*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISJm6P9GwCz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e3e6ad7-5178-464b-dbd5-308c3fd6e54c"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otI30sAExRAG",
        "colab_type": "text"
      },
      "source": [
        "Carga y visualización de la data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uso7wwzEwSOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "589692fe-9302-40b1-861e-7651b721a8e4"
      },
      "source": [
        "(X_train, y_train),(X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhc7Xluzxsvj",
        "colab_type": "code",
        "outputId": "8b1c274c-6a4c-4f4e-be11-a9bf3558fc6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('X_train shape: ',X_train.shape)\n",
        "print('y_train shape: ',y_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape:  (50000, 32, 32, 3)\n",
            "y_train shape:  (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1mf4WPh3Lnf",
        "colab_type": "text"
      },
      "source": [
        "Los objetos a detectar son (labels):\n",
        "\n",
        "\n",
        "*   0: airplane\n",
        "*   1: automobile\n",
        "*   2: bird\n",
        "*   3: cat\n",
        "*   4: deer\n",
        "*   5: dog\n",
        "*   6: frog\n",
        "*   7: horse\n",
        "*   8: ship\n",
        "*   9: truck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2gj7GP5FZZ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "382d3d87-08eb-4bdb-b1fa-1648bd35c075"
      },
      "source": [
        "#visualización de las primeras 6 imágenes del train set\n",
        "for i in range(6):\n",
        "  plt.subplot(330+1+i)\n",
        "  plt.imshow(X_train[i])\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAACvCAYAAABEme2fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwdLoReM4ufw",
        "colab_type": "code",
        "outputId": "e82829f3-6379-4631-9068-457eeded8fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#One hot encoding \n",
        "train_label = to_categorical(y_train)\n",
        "test_label = to_categorical(y_test)\n",
        "train_label"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LycfYD4E9lJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalizar pixeles\n",
        "train_new = X_train.astype('float32')/255.0\n",
        "test_new = X_test.astype('float32')/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0c8MMNTS4Lm",
        "colab_type": "text"
      },
      "source": [
        "# Experimentación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqMfulrj1ZMZ",
        "colab_type": "code",
        "outputId": "e5c92269-008a-4b33-efff-4ba5c3867f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "optimizador = optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_new, train_label, epochs=100,batch_size=64,verbose=1,validation_split=0.4)\n",
        "_, accuracy = model.evaluate(train_new,train_label,verbose=1)\n",
        "print('accuracy es: ',(accuracy*100.0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Train on 30000 samples, validate on 20000 samples\n",
            "Epoch 1/100\n",
            "30000/30000 [==============================] - 19s 630us/step - loss: 1.8839 - acc: 0.3231 - val_loss: 1.6105 - val_acc: 0.4240\n",
            "Epoch 2/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.5073 - acc: 0.4588 - val_loss: 1.4902 - val_acc: 0.4506\n",
            "Epoch 3/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.3585 - acc: 0.5099 - val_loss: 1.3271 - val_acc: 0.5269\n",
            "Epoch 4/100\n",
            "30000/30000 [==============================] - 11s 379us/step - loss: 1.2323 - acc: 0.5600 - val_loss: 1.2381 - val_acc: 0.5592\n",
            "Epoch 5/100\n",
            "30000/30000 [==============================] - 11s 380us/step - loss: 1.1282 - acc: 0.5962 - val_loss: 1.1841 - val_acc: 0.5810\n",
            "Epoch 6/100\n",
            "30000/30000 [==============================] - 11s 377us/step - loss: 1.0498 - acc: 0.6288 - val_loss: 1.0849 - val_acc: 0.6213\n",
            "Epoch 7/100\n",
            "30000/30000 [==============================] - 11s 378us/step - loss: 0.9645 - acc: 0.6637 - val_loss: 1.0674 - val_acc: 0.6260\n",
            "Epoch 8/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 0.8964 - acc: 0.6879 - val_loss: 1.0614 - val_acc: 0.6279\n",
            "Epoch 9/100\n",
            "30000/30000 [==============================] - 11s 380us/step - loss: 0.8385 - acc: 0.7079 - val_loss: 1.0251 - val_acc: 0.6474\n",
            "Epoch 10/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.7730 - acc: 0.7320 - val_loss: 0.9911 - val_acc: 0.6605\n",
            "Epoch 11/100\n",
            "30000/30000 [==============================] - 11s 380us/step - loss: 0.7146 - acc: 0.7554 - val_loss: 0.9871 - val_acc: 0.6677\n",
            "Epoch 12/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.6650 - acc: 0.7703 - val_loss: 1.0223 - val_acc: 0.6597\n",
            "Epoch 13/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.6061 - acc: 0.7883 - val_loss: 1.0336 - val_acc: 0.6655\n",
            "Epoch 14/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 0.5527 - acc: 0.8102 - val_loss: 1.0218 - val_acc: 0.6653\n",
            "Epoch 15/100\n",
            "30000/30000 [==============================] - 11s 379us/step - loss: 0.5026 - acc: 0.8251 - val_loss: 1.0481 - val_acc: 0.6735\n",
            "Epoch 16/100\n",
            "30000/30000 [==============================] - 11s 379us/step - loss: 0.4553 - acc: 0.8444 - val_loss: 1.0257 - val_acc: 0.6786\n",
            "Epoch 17/100\n",
            "30000/30000 [==============================] - 11s 380us/step - loss: 0.4052 - acc: 0.8605 - val_loss: 1.1660 - val_acc: 0.6622\n",
            "Epoch 18/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.3602 - acc: 0.8770 - val_loss: 1.1671 - val_acc: 0.6652\n",
            "Epoch 19/100\n",
            "30000/30000 [==============================] - 11s 379us/step - loss: 0.3058 - acc: 0.8958 - val_loss: 1.1564 - val_acc: 0.6773\n",
            "Epoch 20/100\n",
            "30000/30000 [==============================] - 11s 379us/step - loss: 0.2595 - acc: 0.9117 - val_loss: 1.3043 - val_acc: 0.6732\n",
            "Epoch 21/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.2259 - acc: 0.9225 - val_loss: 1.3625 - val_acc: 0.6623\n",
            "Epoch 22/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.1894 - acc: 0.9355 - val_loss: 1.3140 - val_acc: 0.6768\n",
            "Epoch 23/100\n",
            "30000/30000 [==============================] - 11s 383us/step - loss: 0.1599 - acc: 0.9474 - val_loss: 1.4111 - val_acc: 0.6786\n",
            "Epoch 24/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 0.1260 - acc: 0.9572 - val_loss: 1.4669 - val_acc: 0.6766\n",
            "Epoch 25/100\n",
            "30000/30000 [==============================] - 11s 379us/step - loss: 0.1130 - acc: 0.9617 - val_loss: 1.6173 - val_acc: 0.6684\n",
            "Epoch 26/100\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.1108 - acc: 0.9625 - val_loss: 1.6177 - val_acc: 0.6767\n",
            "Epoch 27/100\n",
            "30000/30000 [==============================] - 11s 381us/step - loss: 0.1033 - acc: 0.9645 - val_loss: 1.7293 - val_acc: 0.6661\n",
            "Epoch 28/100\n",
            "30000/30000 [==============================] - 11s 383us/step - loss: 0.0680 - acc: 0.9781 - val_loss: 1.7253 - val_acc: 0.6786\n",
            "Epoch 29/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 0.0517 - acc: 0.9847 - val_loss: 1.8636 - val_acc: 0.6725\n",
            "Epoch 30/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 0.0584 - acc: 0.9810 - val_loss: 1.8767 - val_acc: 0.6774\n",
            "Epoch 31/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 0.0447 - acc: 0.9860 - val_loss: 2.0029 - val_acc: 0.6798\n",
            "Epoch 32/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 0.0228 - acc: 0.9942 - val_loss: 1.9862 - val_acc: 0.6834\n",
            "Epoch 33/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 0.0154 - acc: 0.9964 - val_loss: 2.0287 - val_acc: 0.6885\n",
            "Epoch 34/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 0.0032 - acc: 0.9999 - val_loss: 2.0807 - val_acc: 0.6910\n",
            "Epoch 35/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 2.1240 - val_acc: 0.6942\n",
            "Epoch 36/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 2.1595 - val_acc: 0.6956\n",
            "Epoch 37/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 8.3512e-04 - acc: 1.0000 - val_loss: 2.1825 - val_acc: 0.6962\n",
            "Epoch 38/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 7.3328e-04 - acc: 1.0000 - val_loss: 2.2049 - val_acc: 0.6964\n",
            "Epoch 39/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 6.4492e-04 - acc: 1.0000 - val_loss: 2.2251 - val_acc: 0.6966\n",
            "Epoch 40/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 5.8487e-04 - acc: 1.0000 - val_loss: 2.2406 - val_acc: 0.6965\n",
            "Epoch 41/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 5.3642e-04 - acc: 1.0000 - val_loss: 2.2571 - val_acc: 0.6966\n",
            "Epoch 42/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 4.9373e-04 - acc: 1.0000 - val_loss: 2.2698 - val_acc: 0.6965\n",
            "Epoch 43/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 4.5870e-04 - acc: 1.0000 - val_loss: 2.2827 - val_acc: 0.6964\n",
            "Epoch 44/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 4.2997e-04 - acc: 1.0000 - val_loss: 2.2943 - val_acc: 0.6965\n",
            "Epoch 45/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 4.0344e-04 - acc: 1.0000 - val_loss: 2.3047 - val_acc: 0.6966\n",
            "Epoch 46/100\n",
            "30000/30000 [==============================] - 12s 388us/step - loss: 3.7991e-04 - acc: 1.0000 - val_loss: 2.3151 - val_acc: 0.6964\n",
            "Epoch 47/100\n",
            "30000/30000 [==============================] - 12s 390us/step - loss: 3.6088e-04 - acc: 1.0000 - val_loss: 2.3241 - val_acc: 0.6967\n",
            "Epoch 48/100\n",
            "30000/30000 [==============================] - 11s 383us/step - loss: 3.4283e-04 - acc: 1.0000 - val_loss: 2.3333 - val_acc: 0.6965\n",
            "Epoch 49/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 3.2662e-04 - acc: 1.0000 - val_loss: 2.3416 - val_acc: 0.6966\n",
            "Epoch 50/100\n",
            "30000/30000 [==============================] - 12s 390us/step - loss: 3.1137e-04 - acc: 1.0000 - val_loss: 2.3485 - val_acc: 0.6968\n",
            "Epoch 51/100\n",
            "30000/30000 [==============================] - 12s 388us/step - loss: 2.9818e-04 - acc: 1.0000 - val_loss: 2.3553 - val_acc: 0.6969\n",
            "Epoch 52/100\n",
            "30000/30000 [==============================] - 12s 388us/step - loss: 2.8670e-04 - acc: 1.0000 - val_loss: 2.3631 - val_acc: 0.6966\n",
            "Epoch 53/100\n",
            "30000/30000 [==============================] - 12s 389us/step - loss: 2.7460e-04 - acc: 1.0000 - val_loss: 2.3706 - val_acc: 0.6965\n",
            "Epoch 54/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 2.6403e-04 - acc: 1.0000 - val_loss: 2.3762 - val_acc: 0.6968\n",
            "Epoch 55/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 2.5466e-04 - acc: 1.0000 - val_loss: 2.3828 - val_acc: 0.6969\n",
            "Epoch 56/100\n",
            "30000/30000 [==============================] - 12s 389us/step - loss: 2.4533e-04 - acc: 1.0000 - val_loss: 2.3886 - val_acc: 0.6968\n",
            "Epoch 57/100\n",
            "30000/30000 [==============================] - 12s 388us/step - loss: 2.3683e-04 - acc: 1.0000 - val_loss: 2.3944 - val_acc: 0.6967\n",
            "Epoch 58/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 2.2970e-04 - acc: 1.0000 - val_loss: 2.4009 - val_acc: 0.6968\n",
            "Epoch 59/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 2.2216e-04 - acc: 1.0000 - val_loss: 2.4062 - val_acc: 0.6966\n",
            "Epoch 60/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 2.1530e-04 - acc: 1.0000 - val_loss: 2.4108 - val_acc: 0.6968\n",
            "Epoch 61/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 2.0880e-04 - acc: 1.0000 - val_loss: 2.4160 - val_acc: 0.6968\n",
            "Epoch 62/100\n",
            "30000/30000 [==============================] - 12s 388us/step - loss: 2.0289e-04 - acc: 1.0000 - val_loss: 2.4211 - val_acc: 0.6967\n",
            "Epoch 63/100\n",
            "30000/30000 [==============================] - 12s 391us/step - loss: 1.9693e-04 - acc: 1.0000 - val_loss: 2.4260 - val_acc: 0.6967\n",
            "Epoch 64/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.9199e-04 - acc: 1.0000 - val_loss: 2.4307 - val_acc: 0.6966\n",
            "Epoch 65/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.8673e-04 - acc: 1.0000 - val_loss: 2.4346 - val_acc: 0.6968\n",
            "Epoch 66/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.8179e-04 - acc: 1.0000 - val_loss: 2.4389 - val_acc: 0.6969\n",
            "Epoch 67/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.7709e-04 - acc: 1.0000 - val_loss: 2.4433 - val_acc: 0.6965\n",
            "Epoch 68/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.7283e-04 - acc: 1.0000 - val_loss: 2.4475 - val_acc: 0.6969\n",
            "Epoch 69/100\n",
            "30000/30000 [==============================] - 11s 380us/step - loss: 1.6832e-04 - acc: 1.0000 - val_loss: 2.4514 - val_acc: 0.6973\n",
            "Epoch 70/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 1.6452e-04 - acc: 1.0000 - val_loss: 2.4555 - val_acc: 0.6972\n",
            "Epoch 71/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.6089e-04 - acc: 1.0000 - val_loss: 2.4590 - val_acc: 0.6969\n",
            "Epoch 72/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.5692e-04 - acc: 1.0000 - val_loss: 2.4631 - val_acc: 0.6971\n",
            "Epoch 73/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 1.5369e-04 - acc: 1.0000 - val_loss: 2.4664 - val_acc: 0.6966\n",
            "Epoch 74/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.5046e-04 - acc: 1.0000 - val_loss: 2.4701 - val_acc: 0.6970\n",
            "Epoch 75/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.4715e-04 - acc: 1.0000 - val_loss: 2.4737 - val_acc: 0.6972\n",
            "Epoch 76/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 1.4399e-04 - acc: 1.0000 - val_loss: 2.4778 - val_acc: 0.6972\n",
            "Epoch 77/100\n",
            "30000/30000 [==============================] - 12s 392us/step - loss: 1.4098e-04 - acc: 1.0000 - val_loss: 2.4808 - val_acc: 0.6974\n",
            "Epoch 78/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 1.3834e-04 - acc: 1.0000 - val_loss: 2.4846 - val_acc: 0.6973\n",
            "Epoch 79/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.3565e-04 - acc: 1.0000 - val_loss: 2.4871 - val_acc: 0.6970\n",
            "Epoch 80/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.3302e-04 - acc: 1.0000 - val_loss: 2.4907 - val_acc: 0.6974\n",
            "Epoch 81/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 1.3045e-04 - acc: 1.0000 - val_loss: 2.4936 - val_acc: 0.6974\n",
            "Epoch 82/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.2793e-04 - acc: 1.0000 - val_loss: 2.4967 - val_acc: 0.6970\n",
            "Epoch 83/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.2576e-04 - acc: 1.0000 - val_loss: 2.5000 - val_acc: 0.6968\n",
            "Epoch 84/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 1.2344e-04 - acc: 1.0000 - val_loss: 2.5029 - val_acc: 0.6969\n",
            "Epoch 85/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.2118e-04 - acc: 1.0000 - val_loss: 2.5058 - val_acc: 0.6974\n",
            "Epoch 86/100\n",
            "30000/30000 [==============================] - 12s 386us/step - loss: 1.1922e-04 - acc: 1.0000 - val_loss: 2.5086 - val_acc: 0.6970\n",
            "Epoch 87/100\n",
            "30000/30000 [==============================] - 11s 383us/step - loss: 1.1720e-04 - acc: 1.0000 - val_loss: 2.5111 - val_acc: 0.6966\n",
            "Epoch 88/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.1510e-04 - acc: 1.0000 - val_loss: 2.5140 - val_acc: 0.6970\n",
            "Epoch 89/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.1331e-04 - acc: 1.0000 - val_loss: 2.5166 - val_acc: 0.6971\n",
            "Epoch 90/100\n",
            "30000/30000 [==============================] - 11s 382us/step - loss: 1.1138e-04 - acc: 1.0000 - val_loss: 2.5193 - val_acc: 0.6973\n",
            "Epoch 91/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.0958e-04 - acc: 1.0000 - val_loss: 2.5215 - val_acc: 0.6972\n",
            "Epoch 92/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.0778e-04 - acc: 1.0000 - val_loss: 2.5248 - val_acc: 0.6972\n",
            "Epoch 93/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.0623e-04 - acc: 1.0000 - val_loss: 2.5274 - val_acc: 0.6969\n",
            "Epoch 94/100\n",
            "30000/30000 [==============================] - 12s 387us/step - loss: 1.0463e-04 - acc: 1.0000 - val_loss: 2.5296 - val_acc: 0.6972\n",
            "Epoch 95/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 1.0303e-04 - acc: 1.0000 - val_loss: 2.5322 - val_acc: 0.6968\n",
            "Epoch 96/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 1.0152e-04 - acc: 1.0000 - val_loss: 2.5347 - val_acc: 0.6966\n",
            "Epoch 97/100\n",
            "30000/30000 [==============================] - 11s 383us/step - loss: 9.9928e-05 - acc: 1.0000 - val_loss: 2.5368 - val_acc: 0.6968\n",
            "Epoch 98/100\n",
            "30000/30000 [==============================] - 12s 388us/step - loss: 9.8602e-05 - acc: 1.0000 - val_loss: 2.5394 - val_acc: 0.6967\n",
            "Epoch 99/100\n",
            "30000/30000 [==============================] - 12s 385us/step - loss: 9.7029e-05 - acc: 1.0000 - val_loss: 2.5415 - val_acc: 0.6973\n",
            "Epoch 100/100\n",
            "30000/30000 [==============================] - 12s 384us/step - loss: 9.5732e-05 - acc: 1.0000 - val_loss: 2.5434 - val_acc: 0.6969\n",
            "50000/50000 [==============================] - 9s 173us/step\n",
            "accuracy es:  87.878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h158d7C2CTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2b7497f-46f2-4d62-a15c-1c162b165f7b"
      },
      "source": [
        "model.evaluate(train_new,train_label,verbose=0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.01741222224897, 0.87878]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6u4dA-MMapp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3dde0b47-df5d-400e-afd2-ece9b6417744"
      },
      "source": [
        "model.evaluate(test_new,test_label,verbose=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6909587539672852, 0.6882]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q54Yc4QsNXWp",
        "colab_type": "text"
      },
      "source": [
        "Dados los valores de model.evalute, se puede observar que hay presencia de overfitting (0.87 -train accuracy- vs 0.68 -test accuracy-. El siguiente paso consiste en aplicar regularización (dropout) para atacar dicho problema y evaluar de nuevo la métrica. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbqoLNogNUUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout(verb):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  optimizador = optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "  model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_new, train_label, epochs=100,batch_size=64,verbose=1,validation_split=0.4)\n",
        "  _, accuracy = model.evaluate(train_new,train_label,verbose=verb)\n",
        "  c,accuracy_test = model.evaluate(test_new,test_label,verbose=verb)\n",
        "  return [accuracy,accuracy_test]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S4H92tdOjjj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3bcce41-5053-405c-88d2-b73c4f2f76bd"
      },
      "source": [
        "dropout(1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 30000 samples, validate on 20000 samples\n",
            "Epoch 1/100\n",
            "30000/30000 [==============================] - 13s 435us/step - loss: 2.2335 - acc: 0.1526 - val_loss: 2.1003 - val_acc: 0.2245\n",
            "Epoch 2/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 2.0244 - acc: 0.2370 - val_loss: 1.8839 - val_acc: 0.3327\n",
            "Epoch 3/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.8726 - acc: 0.2902 - val_loss: 1.7401 - val_acc: 0.3633\n",
            "Epoch 4/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 1.7706 - acc: 0.3299 - val_loss: 1.6605 - val_acc: 0.3829\n",
            "Epoch 5/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 1.7023 - acc: 0.3625 - val_loss: 1.6743 - val_acc: 0.3756\n",
            "Epoch 6/100\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 1.6471 - acc: 0.3863 - val_loss: 1.5401 - val_acc: 0.4361\n",
            "Epoch 7/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 1.6010 - acc: 0.4055 - val_loss: 1.4759 - val_acc: 0.4660\n",
            "Epoch 8/100\n",
            "30000/30000 [==============================] - 12s 401us/step - loss: 1.5458 - acc: 0.4283 - val_loss: 1.4347 - val_acc: 0.4776\n",
            "Epoch 9/100\n",
            "30000/30000 [==============================] - 12s 402us/step - loss: 1.4905 - acc: 0.4516 - val_loss: 1.4336 - val_acc: 0.4807\n",
            "Epoch 10/100\n",
            "30000/30000 [==============================] - 12s 401us/step - loss: 1.4472 - acc: 0.4714 - val_loss: 1.3660 - val_acc: 0.5073\n",
            "Epoch 11/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 1.4074 - acc: 0.4845 - val_loss: 1.3497 - val_acc: 0.5077\n",
            "Epoch 12/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 1.3645 - acc: 0.5031 - val_loss: 1.2769 - val_acc: 0.5358\n",
            "Epoch 13/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 1.3278 - acc: 0.5183 - val_loss: 1.2021 - val_acc: 0.5673\n",
            "Epoch 14/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.2911 - acc: 0.5336 - val_loss: 1.2174 - val_acc: 0.5639\n",
            "Epoch 15/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.2631 - acc: 0.5451 - val_loss: 1.1838 - val_acc: 0.5772\n",
            "Epoch 16/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.2254 - acc: 0.5589 - val_loss: 1.1621 - val_acc: 0.5774\n",
            "Epoch 17/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 1.2025 - acc: 0.5688 - val_loss: 1.0762 - val_acc: 0.6121\n",
            "Epoch 18/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 1.1803 - acc: 0.5759 - val_loss: 1.0878 - val_acc: 0.6157\n",
            "Epoch 19/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.1571 - acc: 0.5849 - val_loss: 1.0517 - val_acc: 0.6267\n",
            "Epoch 20/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.1422 - acc: 0.5888 - val_loss: 1.0400 - val_acc: 0.6299\n",
            "Epoch 21/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 1.1225 - acc: 0.6001 - val_loss: 1.0201 - val_acc: 0.6327\n",
            "Epoch 22/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 1.1082 - acc: 0.6062 - val_loss: 0.9726 - val_acc: 0.6499\n",
            "Epoch 23/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 1.0861 - acc: 0.6163 - val_loss: 0.9570 - val_acc: 0.6605\n",
            "Epoch 24/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 1.0727 - acc: 0.6181 - val_loss: 0.9837 - val_acc: 0.6470\n",
            "Epoch 25/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 1.0540 - acc: 0.6267 - val_loss: 0.9489 - val_acc: 0.6628\n",
            "Epoch 26/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 1.0347 - acc: 0.6318 - val_loss: 0.9308 - val_acc: 0.6655\n",
            "Epoch 27/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 1.0244 - acc: 0.6348 - val_loss: 0.9153 - val_acc: 0.6780\n",
            "Epoch 28/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 1.0145 - acc: 0.6395 - val_loss: 0.9442 - val_acc: 0.6623\n",
            "Epoch 29/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 1.0032 - acc: 0.6456 - val_loss: 0.8991 - val_acc: 0.6820\n",
            "Epoch 30/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.9811 - acc: 0.6528 - val_loss: 0.8919 - val_acc: 0.6792\n",
            "Epoch 31/100\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.9619 - acc: 0.6596 - val_loss: 0.9297 - val_acc: 0.6670\n",
            "Epoch 32/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.9566 - acc: 0.6603 - val_loss: 0.8584 - val_acc: 0.6942\n",
            "Epoch 33/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.9436 - acc: 0.6668 - val_loss: 0.8579 - val_acc: 0.6941\n",
            "Epoch 34/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.9334 - acc: 0.6676 - val_loss: 0.8481 - val_acc: 0.6966\n",
            "Epoch 35/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 0.9251 - acc: 0.6759 - val_loss: 0.8259 - val_acc: 0.7022\n",
            "Epoch 36/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.9119 - acc: 0.6778 - val_loss: 0.8174 - val_acc: 0.7085\n",
            "Epoch 37/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.8917 - acc: 0.6843 - val_loss: 0.7995 - val_acc: 0.7156\n",
            "Epoch 38/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.8929 - acc: 0.6855 - val_loss: 0.8123 - val_acc: 0.7098\n",
            "Epoch 39/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.8808 - acc: 0.6898 - val_loss: 0.7946 - val_acc: 0.7178\n",
            "Epoch 40/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.8714 - acc: 0.6928 - val_loss: 0.7964 - val_acc: 0.7161\n",
            "Epoch 41/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.8593 - acc: 0.6966 - val_loss: 0.7680 - val_acc: 0.7257\n",
            "Epoch 42/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.8518 - acc: 0.7006 - val_loss: 0.7742 - val_acc: 0.7246\n",
            "Epoch 43/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.8368 - acc: 0.7070 - val_loss: 0.7705 - val_acc: 0.7282\n",
            "Epoch 44/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.8386 - acc: 0.7020 - val_loss: 0.7831 - val_acc: 0.7232\n",
            "Epoch 45/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.8277 - acc: 0.7099 - val_loss: 0.7859 - val_acc: 0.7218\n",
            "Epoch 46/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.8207 - acc: 0.7104 - val_loss: 0.7585 - val_acc: 0.7317\n",
            "Epoch 47/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 0.8137 - acc: 0.7139 - val_loss: 0.7680 - val_acc: 0.7277\n",
            "Epoch 48/100\n",
            "30000/30000 [==============================] - 12s 402us/step - loss: 0.8038 - acc: 0.7159 - val_loss: 0.7440 - val_acc: 0.7378\n",
            "Epoch 49/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.7887 - acc: 0.7224 - val_loss: 0.7514 - val_acc: 0.7351\n",
            "Epoch 50/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.7847 - acc: 0.7231 - val_loss: 0.7379 - val_acc: 0.7395\n",
            "Epoch 51/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.7751 - acc: 0.7297 - val_loss: 0.7830 - val_acc: 0.7246\n",
            "Epoch 52/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.7718 - acc: 0.7286 - val_loss: 0.7327 - val_acc: 0.7410\n",
            "Epoch 53/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.7695 - acc: 0.7306 - val_loss: 0.7194 - val_acc: 0.7480\n",
            "Epoch 54/100\n",
            "30000/30000 [==============================] - 12s 409us/step - loss: 0.7550 - acc: 0.7360 - val_loss: 0.7330 - val_acc: 0.7433\n",
            "Epoch 55/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.7555 - acc: 0.7358 - val_loss: 0.7215 - val_acc: 0.7474\n",
            "Epoch 56/100\n",
            "30000/30000 [==============================] - 12s 409us/step - loss: 0.7368 - acc: 0.7374 - val_loss: 0.7036 - val_acc: 0.7516\n",
            "Epoch 57/100\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.7419 - acc: 0.7389 - val_loss: 0.6946 - val_acc: 0.7549\n",
            "Epoch 58/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.7267 - acc: 0.7451 - val_loss: 0.6818 - val_acc: 0.7609\n",
            "Epoch 59/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.7245 - acc: 0.7429 - val_loss: 0.7162 - val_acc: 0.7491\n",
            "Epoch 60/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 0.7146 - acc: 0.7510 - val_loss: 0.6986 - val_acc: 0.7519\n",
            "Epoch 61/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.7073 - acc: 0.7516 - val_loss: 0.6770 - val_acc: 0.7597\n",
            "Epoch 62/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.7006 - acc: 0.7543 - val_loss: 0.6832 - val_acc: 0.7617\n",
            "Epoch 63/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.6979 - acc: 0.7576 - val_loss: 0.6715 - val_acc: 0.7652\n",
            "Epoch 64/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.6956 - acc: 0.7557 - val_loss: 0.6873 - val_acc: 0.7612\n",
            "Epoch 65/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 0.6918 - acc: 0.7561 - val_loss: 0.6633 - val_acc: 0.7683\n",
            "Epoch 66/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.6797 - acc: 0.7599 - val_loss: 0.6943 - val_acc: 0.7560\n",
            "Epoch 67/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.6762 - acc: 0.7616 - val_loss: 0.6654 - val_acc: 0.7690\n",
            "Epoch 68/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.6793 - acc: 0.7633 - val_loss: 0.6752 - val_acc: 0.7635\n",
            "Epoch 69/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.6658 - acc: 0.7659 - val_loss: 0.6504 - val_acc: 0.7730\n",
            "Epoch 70/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.6623 - acc: 0.7668 - val_loss: 0.6602 - val_acc: 0.7724\n",
            "Epoch 71/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 0.6543 - acc: 0.7678 - val_loss: 0.6516 - val_acc: 0.7705\n",
            "Epoch 72/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.6406 - acc: 0.7761 - val_loss: 0.6446 - val_acc: 0.7760\n",
            "Epoch 73/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 0.6405 - acc: 0.7748 - val_loss: 0.6678 - val_acc: 0.7707\n",
            "Epoch 74/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.6398 - acc: 0.7770 - val_loss: 0.6477 - val_acc: 0.7745\n",
            "Epoch 75/100\n",
            "30000/30000 [==============================] - 12s 408us/step - loss: 0.6287 - acc: 0.7797 - val_loss: 0.6482 - val_acc: 0.7740\n",
            "Epoch 76/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.6291 - acc: 0.7807 - val_loss: 0.6344 - val_acc: 0.7790\n",
            "Epoch 77/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 0.6179 - acc: 0.7802 - val_loss: 0.6453 - val_acc: 0.7741\n",
            "Epoch 78/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.6173 - acc: 0.7833 - val_loss: 0.6239 - val_acc: 0.7835\n",
            "Epoch 79/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 0.6163 - acc: 0.7831 - val_loss: 0.6192 - val_acc: 0.7845\n",
            "Epoch 80/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.6102 - acc: 0.7859 - val_loss: 0.6460 - val_acc: 0.7767\n",
            "Epoch 81/100\n",
            "30000/30000 [==============================] - 12s 409us/step - loss: 0.6074 - acc: 0.7860 - val_loss: 0.6216 - val_acc: 0.7851\n",
            "Epoch 82/100\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.5982 - acc: 0.7877 - val_loss: 0.6338 - val_acc: 0.7802\n",
            "Epoch 83/100\n",
            "30000/30000 [==============================] - 12s 408us/step - loss: 0.5949 - acc: 0.7886 - val_loss: 0.6271 - val_acc: 0.7835\n",
            "Epoch 84/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5915 - acc: 0.7925 - val_loss: 0.6356 - val_acc: 0.7827\n",
            "Epoch 85/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 0.5866 - acc: 0.7925 - val_loss: 0.6143 - val_acc: 0.7891\n",
            "Epoch 86/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 0.5847 - acc: 0.7960 - val_loss: 0.6204 - val_acc: 0.7880\n",
            "Epoch 87/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.5766 - acc: 0.7967 - val_loss: 0.6034 - val_acc: 0.7910\n",
            "Epoch 88/100\n",
            "30000/30000 [==============================] - 12s 407us/step - loss: 0.5700 - acc: 0.7985 - val_loss: 0.6331 - val_acc: 0.7843\n",
            "Epoch 89/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.5649 - acc: 0.8011 - val_loss: 0.6062 - val_acc: 0.7916\n",
            "Epoch 90/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5641 - acc: 0.8013 - val_loss: 0.6402 - val_acc: 0.7804\n",
            "Epoch 91/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.5613 - acc: 0.8010 - val_loss: 0.6180 - val_acc: 0.7873\n",
            "Epoch 92/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5519 - acc: 0.8045 - val_loss: 0.6079 - val_acc: 0.7917\n",
            "Epoch 93/100\n",
            "30000/30000 [==============================] - 12s 404us/step - loss: 0.5507 - acc: 0.8071 - val_loss: 0.6433 - val_acc: 0.7814\n",
            "Epoch 94/100\n",
            "30000/30000 [==============================] - 12s 403us/step - loss: 0.5487 - acc: 0.8041 - val_loss: 0.6103 - val_acc: 0.7909\n",
            "Epoch 95/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.5401 - acc: 0.8066 - val_loss: 0.5961 - val_acc: 0.7959\n",
            "Epoch 96/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5411 - acc: 0.8081 - val_loss: 0.6088 - val_acc: 0.7943\n",
            "Epoch 97/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5316 - acc: 0.8121 - val_loss: 0.5991 - val_acc: 0.7937\n",
            "Epoch 98/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5342 - acc: 0.8119 - val_loss: 0.6302 - val_acc: 0.7864\n",
            "Epoch 99/100\n",
            "30000/30000 [==============================] - 12s 405us/step - loss: 0.5332 - acc: 0.8101 - val_loss: 0.5990 - val_acc: 0.7971\n",
            "Epoch 100/100\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.5190 - acc: 0.8154 - val_loss: 0.5992 - val_acc: 0.7980\n",
            "50000/50000 [==============================] - 9s 175us/step\n",
            "10000/10000 [==============================] - 2s 194us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8577, 0.7938]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAIvV52NcC73",
        "colab_type": "text"
      },
      "source": [
        "Aplicando dropout, el gap entre los accuracy disminuyó (0.85 vs 0.79), lo cual es bueno para el modelo ya que se redujo overfitting. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb9a52pOcdZj",
        "colab_type": "text"
      },
      "source": [
        "En aras de mejorar la métrica, ahora se utiliza GD con Nesterov y 150 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tugd3GolOsiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout2(epoch):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  optimizador = optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True)\n",
        "  model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_new, train_label, epochs=epoch,batch_size=64,verbose=1,validation_split=0.4)\n",
        "  _, accuracy = model.evaluate(train_new,train_label,verbose=0)\n",
        "  c,accuracy_test = model.evaluate(test_new,test_label,verbose=0)\n",
        "  return [accuracy,accuracy_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSvMBLgqcvY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "412698b3-9244-46ec-c55a-a0c98a0ac350"
      },
      "source": [
        "dropout2(150)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 30000 samples, validate on 20000 samples\n",
            "Epoch 1/150\n",
            "30000/30000 [==============================] - 14s 459us/step - loss: 2.2559 - acc: 0.1448 - val_loss: 2.0910 - val_acc: 0.2392\n",
            "Epoch 2/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 2.0275 - acc: 0.2348 - val_loss: 1.8631 - val_acc: 0.3258\n",
            "Epoch 3/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 1.8468 - acc: 0.3050 - val_loss: 1.7123 - val_acc: 0.3836\n",
            "Epoch 4/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 1.7342 - acc: 0.3483 - val_loss: 1.6005 - val_acc: 0.4107\n",
            "Epoch 5/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 1.6641 - acc: 0.3788 - val_loss: 1.5398 - val_acc: 0.4345\n",
            "Epoch 6/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 1.6028 - acc: 0.4059 - val_loss: 1.5012 - val_acc: 0.4553\n",
            "Epoch 7/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 1.5496 - acc: 0.4291 - val_loss: 1.4309 - val_acc: 0.4802\n",
            "Epoch 8/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 1.4976 - acc: 0.4542 - val_loss: 1.3861 - val_acc: 0.4898\n",
            "Epoch 9/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 1.4609 - acc: 0.4667 - val_loss: 1.3984 - val_acc: 0.4896\n",
            "Epoch 10/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 1.4243 - acc: 0.4797 - val_loss: 1.3242 - val_acc: 0.5228\n",
            "Epoch 11/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.3930 - acc: 0.4935 - val_loss: 1.3248 - val_acc: 0.5167\n",
            "Epoch 12/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.3684 - acc: 0.5044 - val_loss: 1.2597 - val_acc: 0.5456\n",
            "Epoch 13/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.3308 - acc: 0.5152 - val_loss: 1.2300 - val_acc: 0.5598\n",
            "Epoch 14/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 1.3027 - acc: 0.5369 - val_loss: 1.1781 - val_acc: 0.5786\n",
            "Epoch 15/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 1.2790 - acc: 0.5408 - val_loss: 1.1659 - val_acc: 0.5795\n",
            "Epoch 16/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 1.2413 - acc: 0.5570 - val_loss: 1.1344 - val_acc: 0.5959\n",
            "Epoch 17/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.2128 - acc: 0.5674 - val_loss: 1.1193 - val_acc: 0.6009\n",
            "Epoch 18/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 1.1894 - acc: 0.5776 - val_loss: 1.0677 - val_acc: 0.6217\n",
            "Epoch 19/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 1.1706 - acc: 0.5851 - val_loss: 1.0731 - val_acc: 0.6161\n",
            "Epoch 20/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 1.1436 - acc: 0.5945 - val_loss: 1.0405 - val_acc: 0.6288\n",
            "Epoch 21/150\n",
            "30000/30000 [==============================] - 13s 424us/step - loss: 1.1178 - acc: 0.6063 - val_loss: 1.0272 - val_acc: 0.6308\n",
            "Epoch 22/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.0964 - acc: 0.6094 - val_loss: 1.0051 - val_acc: 0.6439\n",
            "Epoch 23/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.0682 - acc: 0.6191 - val_loss: 0.9465 - val_acc: 0.6635\n",
            "Epoch 24/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 1.0524 - acc: 0.6298 - val_loss: 0.9344 - val_acc: 0.6677\n",
            "Epoch 25/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 1.0343 - acc: 0.6379 - val_loss: 0.9275 - val_acc: 0.6697\n",
            "Epoch 26/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.0167 - acc: 0.6405 - val_loss: 0.9092 - val_acc: 0.6795\n",
            "Epoch 27/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.9956 - acc: 0.6506 - val_loss: 0.8791 - val_acc: 0.6878\n",
            "Epoch 28/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.9837 - acc: 0.6508 - val_loss: 0.8746 - val_acc: 0.6881\n",
            "Epoch 29/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.9668 - acc: 0.6618 - val_loss: 0.8665 - val_acc: 0.6953\n",
            "Epoch 30/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.9571 - acc: 0.6640 - val_loss: 0.8551 - val_acc: 0.6974\n",
            "Epoch 31/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.9428 - acc: 0.6667 - val_loss: 0.8370 - val_acc: 0.6998\n",
            "Epoch 32/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.9240 - acc: 0.6746 - val_loss: 0.8288 - val_acc: 0.7085\n",
            "Epoch 33/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.9177 - acc: 0.6812 - val_loss: 0.8050 - val_acc: 0.7149\n",
            "Epoch 34/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.9049 - acc: 0.6798 - val_loss: 0.8059 - val_acc: 0.7148\n",
            "Epoch 35/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.8924 - acc: 0.6865 - val_loss: 0.7991 - val_acc: 0.7163\n",
            "Epoch 36/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.8783 - acc: 0.6907 - val_loss: 0.7899 - val_acc: 0.7199\n",
            "Epoch 37/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.8678 - acc: 0.6948 - val_loss: 0.7840 - val_acc: 0.7206\n",
            "Epoch 38/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.8619 - acc: 0.6990 - val_loss: 0.7974 - val_acc: 0.7170\n",
            "Epoch 39/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.8440 - acc: 0.7047 - val_loss: 0.7564 - val_acc: 0.7315\n",
            "Epoch 40/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 0.8431 - acc: 0.7036 - val_loss: 0.7713 - val_acc: 0.7260\n",
            "Epoch 41/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 0.8277 - acc: 0.7108 - val_loss: 0.7592 - val_acc: 0.7311\n",
            "Epoch 42/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.8200 - acc: 0.7142 - val_loss: 0.7461 - val_acc: 0.7353\n",
            "Epoch 43/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.8120 - acc: 0.7160 - val_loss: 0.7565 - val_acc: 0.7319\n",
            "Epoch 44/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.8096 - acc: 0.7162 - val_loss: 0.7574 - val_acc: 0.7327\n",
            "Epoch 45/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.7944 - acc: 0.7214 - val_loss: 0.7572 - val_acc: 0.7320\n",
            "Epoch 46/150\n",
            "30000/30000 [==============================] - 13s 424us/step - loss: 0.7857 - acc: 0.7231 - val_loss: 0.7288 - val_acc: 0.7430\n",
            "Epoch 47/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.7816 - acc: 0.7268 - val_loss: 0.7305 - val_acc: 0.7423\n",
            "Epoch 48/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7737 - acc: 0.7286 - val_loss: 0.7247 - val_acc: 0.7426\n",
            "Epoch 49/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.7650 - acc: 0.7313 - val_loss: 0.7689 - val_acc: 0.7321\n",
            "Epoch 50/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.7596 - acc: 0.7335 - val_loss: 0.7061 - val_acc: 0.7520\n",
            "Epoch 51/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.7551 - acc: 0.7339 - val_loss: 0.6981 - val_acc: 0.7511\n",
            "Epoch 52/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7454 - acc: 0.7351 - val_loss: 0.6879 - val_acc: 0.7579\n",
            "Epoch 53/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.7354 - acc: 0.7420 - val_loss: 0.7024 - val_acc: 0.7527\n",
            "Epoch 54/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.7329 - acc: 0.7445 - val_loss: 0.6963 - val_acc: 0.7546\n",
            "Epoch 55/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.7230 - acc: 0.7451 - val_loss: 0.7069 - val_acc: 0.7508\n",
            "Epoch 56/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.7167 - acc: 0.7472 - val_loss: 0.6832 - val_acc: 0.7603\n",
            "Epoch 57/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7123 - acc: 0.7500 - val_loss: 0.6657 - val_acc: 0.7663\n",
            "Epoch 58/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.6998 - acc: 0.7530 - val_loss: 0.6686 - val_acc: 0.7651\n",
            "Epoch 59/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7008 - acc: 0.7537 - val_loss: 0.6807 - val_acc: 0.7609\n",
            "Epoch 60/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.6866 - acc: 0.7578 - val_loss: 0.6811 - val_acc: 0.7622\n",
            "Epoch 61/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.6821 - acc: 0.7600 - val_loss: 0.6554 - val_acc: 0.7688\n",
            "Epoch 62/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.6794 - acc: 0.7631 - val_loss: 0.6550 - val_acc: 0.7691\n",
            "Epoch 63/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.6804 - acc: 0.7636 - val_loss: 0.6602 - val_acc: 0.7696\n",
            "Epoch 64/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.6691 - acc: 0.7648 - val_loss: 0.6691 - val_acc: 0.7666\n",
            "Epoch 65/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.6637 - acc: 0.7669 - val_loss: 0.6538 - val_acc: 0.7736\n",
            "Epoch 66/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.6573 - acc: 0.7685 - val_loss: 0.6523 - val_acc: 0.7726\n",
            "Epoch 67/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.6525 - acc: 0.7707 - val_loss: 0.6384 - val_acc: 0.7748\n",
            "Epoch 68/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.6441 - acc: 0.7726 - val_loss: 0.6587 - val_acc: 0.7692\n",
            "Epoch 69/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.6452 - acc: 0.7747 - val_loss: 0.6418 - val_acc: 0.7782\n",
            "Epoch 70/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.6338 - acc: 0.7763 - val_loss: 0.6417 - val_acc: 0.7758\n",
            "Epoch 71/150\n",
            "30000/30000 [==============================] - 13s 421us/step - loss: 0.6299 - acc: 0.7796 - val_loss: 0.6513 - val_acc: 0.7726\n",
            "Epoch 72/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.6279 - acc: 0.7770 - val_loss: 0.6413 - val_acc: 0.7766\n",
            "Epoch 73/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.6242 - acc: 0.7814 - val_loss: 0.6239 - val_acc: 0.7818\n",
            "Epoch 74/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.6167 - acc: 0.7822 - val_loss: 0.6332 - val_acc: 0.7804\n",
            "Epoch 75/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.6158 - acc: 0.7841 - val_loss: 0.6367 - val_acc: 0.7817\n",
            "Epoch 76/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.6095 - acc: 0.7841 - val_loss: 0.6263 - val_acc: 0.7849\n",
            "Epoch 77/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.5980 - acc: 0.7888 - val_loss: 0.6307 - val_acc: 0.7825\n",
            "Epoch 78/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.6025 - acc: 0.7893 - val_loss: 0.6209 - val_acc: 0.7849\n",
            "Epoch 79/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.5904 - acc: 0.7905 - val_loss: 0.6160 - val_acc: 0.7877\n",
            "Epoch 80/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.5928 - acc: 0.7926 - val_loss: 0.6499 - val_acc: 0.7767\n",
            "Epoch 81/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.5851 - acc: 0.7936 - val_loss: 0.6198 - val_acc: 0.7885\n",
            "Epoch 82/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5784 - acc: 0.7948 - val_loss: 0.6324 - val_acc: 0.7788\n",
            "Epoch 83/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.5718 - acc: 0.7997 - val_loss: 0.6163 - val_acc: 0.7898\n",
            "Epoch 84/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5681 - acc: 0.7994 - val_loss: 0.6265 - val_acc: 0.7901\n",
            "Epoch 85/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.5668 - acc: 0.8011 - val_loss: 0.6165 - val_acc: 0.7920\n",
            "Epoch 86/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.5570 - acc: 0.8025 - val_loss: 0.6198 - val_acc: 0.7896\n",
            "Epoch 87/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5593 - acc: 0.8041 - val_loss: 0.6080 - val_acc: 0.7899\n",
            "Epoch 88/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.5509 - acc: 0.8043 - val_loss: 0.6051 - val_acc: 0.7923\n",
            "Epoch 89/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5529 - acc: 0.8058 - val_loss: 0.6013 - val_acc: 0.7951\n",
            "Epoch 90/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5468 - acc: 0.8054 - val_loss: 0.6115 - val_acc: 0.7907\n",
            "Epoch 91/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5386 - acc: 0.8098 - val_loss: 0.5986 - val_acc: 0.7976\n",
            "Epoch 92/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5371 - acc: 0.8088 - val_loss: 0.6012 - val_acc: 0.7944\n",
            "Epoch 93/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.5347 - acc: 0.8084 - val_loss: 0.6248 - val_acc: 0.7902\n",
            "Epoch 94/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.5231 - acc: 0.8163 - val_loss: 0.6079 - val_acc: 0.7964\n",
            "Epoch 95/150\n",
            "30000/30000 [==============================] - 13s 422us/step - loss: 0.5276 - acc: 0.8136 - val_loss: 0.5902 - val_acc: 0.7986\n",
            "Epoch 96/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.5205 - acc: 0.8153 - val_loss: 0.5876 - val_acc: 0.7994\n",
            "Epoch 97/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5190 - acc: 0.8174 - val_loss: 0.6501 - val_acc: 0.7810\n",
            "Epoch 98/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.5133 - acc: 0.8175 - val_loss: 0.5927 - val_acc: 0.7991\n",
            "Epoch 99/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.5119 - acc: 0.8159 - val_loss: 0.5817 - val_acc: 0.8044\n",
            "Epoch 100/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.5078 - acc: 0.8212 - val_loss: 0.5917 - val_acc: 0.7994\n",
            "Epoch 101/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4977 - acc: 0.8240 - val_loss: 0.5998 - val_acc: 0.7970\n",
            "Epoch 102/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.5002 - acc: 0.8215 - val_loss: 0.5890 - val_acc: 0.8025\n",
            "Epoch 103/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.5026 - acc: 0.8238 - val_loss: 0.5989 - val_acc: 0.7999\n",
            "Epoch 104/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.4966 - acc: 0.8257 - val_loss: 0.5974 - val_acc: 0.7993\n",
            "Epoch 105/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4889 - acc: 0.8286 - val_loss: 0.5734 - val_acc: 0.8057\n",
            "Epoch 106/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.4829 - acc: 0.8282 - val_loss: 0.5906 - val_acc: 0.8016\n",
            "Epoch 107/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.4808 - acc: 0.8293 - val_loss: 0.5916 - val_acc: 0.8025\n",
            "Epoch 108/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.4848 - acc: 0.8294 - val_loss: 0.5822 - val_acc: 0.8070\n",
            "Epoch 109/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.4750 - acc: 0.8326 - val_loss: 0.5764 - val_acc: 0.8057\n",
            "Epoch 110/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4732 - acc: 0.8321 - val_loss: 0.5791 - val_acc: 0.8058\n",
            "Epoch 111/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.4715 - acc: 0.8331 - val_loss: 0.5928 - val_acc: 0.8012\n",
            "Epoch 112/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4619 - acc: 0.8364 - val_loss: 0.5730 - val_acc: 0.8081\n",
            "Epoch 113/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.4643 - acc: 0.8335 - val_loss: 0.5774 - val_acc: 0.8061\n",
            "Epoch 114/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.4583 - acc: 0.8373 - val_loss: 0.5855 - val_acc: 0.8054\n",
            "Epoch 115/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.4586 - acc: 0.8370 - val_loss: 0.5718 - val_acc: 0.8082\n",
            "Epoch 116/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4510 - acc: 0.8371 - val_loss: 0.5884 - val_acc: 0.8050\n",
            "Epoch 117/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.4550 - acc: 0.8392 - val_loss: 0.5795 - val_acc: 0.8084\n",
            "Epoch 118/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.4549 - acc: 0.8383 - val_loss: 0.5721 - val_acc: 0.8108\n",
            "Epoch 119/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.4452 - acc: 0.8411 - val_loss: 0.5819 - val_acc: 0.8075\n",
            "Epoch 120/150\n",
            "30000/30000 [==============================] - 13s 421us/step - loss: 0.4369 - acc: 0.8449 - val_loss: 0.5810 - val_acc: 0.8091\n",
            "Epoch 121/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.4380 - acc: 0.8432 - val_loss: 0.5675 - val_acc: 0.8102\n",
            "Epoch 122/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.4358 - acc: 0.8448 - val_loss: 0.5816 - val_acc: 0.8059\n",
            "Epoch 123/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.4368 - acc: 0.8436 - val_loss: 0.5852 - val_acc: 0.8088\n",
            "Epoch 124/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4242 - acc: 0.8481 - val_loss: 0.5861 - val_acc: 0.8103\n",
            "Epoch 125/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4274 - acc: 0.8486 - val_loss: 0.5791 - val_acc: 0.8117\n",
            "Epoch 126/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4195 - acc: 0.8501 - val_loss: 0.5935 - val_acc: 0.8077\n",
            "Epoch 127/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4190 - acc: 0.8501 - val_loss: 0.5714 - val_acc: 0.8121\n",
            "Epoch 128/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4164 - acc: 0.8531 - val_loss: 0.5807 - val_acc: 0.8077\n",
            "Epoch 129/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.4137 - acc: 0.8507 - val_loss: 0.5778 - val_acc: 0.8143\n",
            "Epoch 130/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.4088 - acc: 0.8575 - val_loss: 0.6048 - val_acc: 0.8085\n",
            "Epoch 131/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4096 - acc: 0.8526 - val_loss: 0.5986 - val_acc: 0.8093\n",
            "Epoch 132/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.4068 - acc: 0.8540 - val_loss: 0.5774 - val_acc: 0.8123\n",
            "Epoch 133/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.4062 - acc: 0.8559 - val_loss: 0.5614 - val_acc: 0.8149\n",
            "Epoch 134/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.3999 - acc: 0.8569 - val_loss: 0.6068 - val_acc: 0.8044\n",
            "Epoch 135/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.4072 - acc: 0.8546 - val_loss: 0.5729 - val_acc: 0.8138\n",
            "Epoch 136/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.3928 - acc: 0.8585 - val_loss: 0.5866 - val_acc: 0.8104\n",
            "Epoch 137/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.3969 - acc: 0.8570 - val_loss: 0.5910 - val_acc: 0.8116\n",
            "Epoch 138/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 0.3850 - acc: 0.8623 - val_loss: 0.5939 - val_acc: 0.8126\n",
            "Epoch 139/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.3902 - acc: 0.8627 - val_loss: 0.5891 - val_acc: 0.8115\n",
            "Epoch 140/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.3822 - acc: 0.8642 - val_loss: 0.5762 - val_acc: 0.8159\n",
            "Epoch 141/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.3870 - acc: 0.8617 - val_loss: 0.6154 - val_acc: 0.8091\n",
            "Epoch 142/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.3796 - acc: 0.8644 - val_loss: 0.5693 - val_acc: 0.8171\n",
            "Epoch 143/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.3757 - acc: 0.8651 - val_loss: 0.5683 - val_acc: 0.8170\n",
            "Epoch 144/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.3746 - acc: 0.8668 - val_loss: 0.6086 - val_acc: 0.8097\n",
            "Epoch 145/150\n",
            "30000/30000 [==============================] - 13s 425us/step - loss: 0.3738 - acc: 0.8657 - val_loss: 0.5700 - val_acc: 0.8169\n",
            "Epoch 146/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.3674 - acc: 0.8688 - val_loss: 0.5893 - val_acc: 0.8161\n",
            "Epoch 147/150\n",
            "30000/30000 [==============================] - 13s 417us/step - loss: 0.3714 - acc: 0.8676 - val_loss: 0.5822 - val_acc: 0.8161\n",
            "Epoch 148/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.3636 - acc: 0.8682 - val_loss: 0.5964 - val_acc: 0.8151\n",
            "Epoch 149/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.3625 - acc: 0.8704 - val_loss: 0.6024 - val_acc: 0.8136\n",
            "Epoch 150/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.3645 - acc: 0.8681 - val_loss: 0.5826 - val_acc: 0.8177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.90166, 0.8119]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf2C_MI7c45B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout3(epoch):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  #optimizador = optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True)\n",
        "  model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_new, train_label, epochs=epoch,batch_size=64,verbose=0,validation_split=0.4)\n",
        "  _, accuracy = model.evaluate(train_new,train_label,verbose=0)\n",
        "  c,accuracy_test = model.evaluate(test_new,test_label,verbose=0)\n",
        "  return [accuracy,accuracy_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkZnVaMsutCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6644743-b606-41fc-a3f4-bdada29bacda"
      },
      "source": [
        "dropout3(150)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.91712, 0.8038]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyEA5wzQ29ep",
        "colab_type": "text"
      },
      "source": [
        "En los últimos 2 casos, la métrica mejoró. En el segundo caso se utilizó adagrad como optimizador. Ahora se prueba de nuevo Nesterov con distinto learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcGYksycuxS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout4(epoch):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  optimizador = optimizers.SGD(lr=0.01, momentum=0.9,nesterov=True)\n",
        "  model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_new, train_label, epochs=epoch,batch_size=64,verbose=0,validation_split=0.4)\n",
        "  _, accuracy = model.evaluate(train_new,train_label,verbose=0)\n",
        "  c,accuracy_test = model.evaluate(test_new,test_label,verbose=0)\n",
        "  return [accuracy,accuracy_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnaTFPKT38Hx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3541bc62-bb12-4546-c39c-f51e26573072"
      },
      "source": [
        "dropout4(150)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.89912, 0.8052]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erw2R3-wU9mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout5(epoch):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='truncated_normal', padding='same', input_shape=(32, 32, 3))) #se prueba otro inicializador\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='truncated_normal', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='truncated_normal', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='truncated_normal', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='truncated_normal', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='truncated_normal', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='truncated_normal'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  optimizador = optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True)\n",
        "  model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_new, train_label, epochs=epoch,batch_size=64,verbose=0,validation_split=0.4)\n",
        "  _, accuracy = model.evaluate(train_new,train_label,verbose=0)\n",
        "  c,accuracy_test = model.evaluate(test_new,test_label,verbose=0)\n",
        "  return [accuracy,accuracy_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiZKGs9tWmRs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "51fc07cc-d31a-4c27-87f1-ea9ec6c78fc2"
      },
      "source": [
        "dropout5(100)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.87574, 0.7994]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xokFHFpffud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10720a37-5a33-445b-ae06-1bc23dfa2b63"
      },
      "source": [
        "dropout5(150)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.92376, 0.8216]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUvpUltqETOR",
        "colab_type": "text"
      },
      "source": [
        "**Comentario/Conclusión**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKEiU3AICuzC",
        "colab_type": "text"
      },
      "source": [
        "Varias pruebas arrojaron incrementos en el accuracy tanto de train y test. No obstante, el que mejor desempeño tuvo entre todas las pruebas fue la función llamada \"dropout2\" (0.9 train vs 0.81 test). Este resultado se logró aplicando dropout, acelearando el gradiente con Nesterov y aumentando el número de epochs a 150. \n",
        "\n",
        "No obstante, hay un factor importante: en la función \"dropout1\" el gap entre los accuracys fue bajo pero los valores fueron menores. Hay dos posibles restos/soluciones para resolver el problema en general: 1) reducir el gap en el \"dropou2\" para atacar aún mas overfitting o bien 2) seguir experimentado para mejorar el \"dropout1\" para alzanzar mayores niveles de accuracy.\n",
        "\n",
        "De igual manera, en la última prueba se experimentó con Batch Normalization y el accuracy mejoró el valor en ambos sets, pero diferencia también lo hizo (10 puntos). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y527heSe53T",
        "colab_type": "text"
      },
      "source": [
        "# **Modelo final**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K53NTJTNFNKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f40ca9f-add2-4a0d-f502-c9c8f47f2452"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "optimizador = optimizers.SGD(lr=0.001, momentum=0.9,nesterov=True)\n",
        "model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_new, train_label, epochs=150,batch_size=64,verbose=1,validation_split=0.4)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 30000 samples, validate on 20000 samples\n",
            "Epoch 1/150\n",
            "30000/30000 [==============================] - 14s 452us/step - loss: 2.2638 - acc: 0.1377 - val_loss: 2.1126 - val_acc: 0.2143\n",
            "Epoch 2/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 2.0419 - acc: 0.2367 - val_loss: 1.8759 - val_acc: 0.3322\n",
            "Epoch 3/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.8474 - acc: 0.3053 - val_loss: 1.7680 - val_acc: 0.3377\n",
            "Epoch 4/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.7354 - acc: 0.3530 - val_loss: 1.6216 - val_acc: 0.4072\n",
            "Epoch 5/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 1.6463 - acc: 0.3863 - val_loss: 1.5143 - val_acc: 0.4497\n",
            "Epoch 6/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.5883 - acc: 0.4100 - val_loss: 1.4729 - val_acc: 0.4616\n",
            "Epoch 7/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 1.5349 - acc: 0.4345 - val_loss: 1.4219 - val_acc: 0.4854\n",
            "Epoch 8/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 1.4796 - acc: 0.4538 - val_loss: 1.3936 - val_acc: 0.4995\n",
            "Epoch 9/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.4552 - acc: 0.4682 - val_loss: 1.3673 - val_acc: 0.5089\n",
            "Epoch 10/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 1.4148 - acc: 0.4804 - val_loss: 1.3006 - val_acc: 0.5354\n",
            "Epoch 11/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.3860 - acc: 0.4950 - val_loss: 1.2761 - val_acc: 0.5406\n",
            "Epoch 12/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 1.3509 - acc: 0.5106 - val_loss: 1.2284 - val_acc: 0.5596\n",
            "Epoch 13/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 1.3119 - acc: 0.5240 - val_loss: 1.1933 - val_acc: 0.5728\n",
            "Epoch 14/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.2864 - acc: 0.5372 - val_loss: 1.1538 - val_acc: 0.5857\n",
            "Epoch 15/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 1.2459 - acc: 0.5485 - val_loss: 1.1193 - val_acc: 0.5987\n",
            "Epoch 16/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 1.2153 - acc: 0.5653 - val_loss: 1.0919 - val_acc: 0.6109\n",
            "Epoch 17/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 1.1890 - acc: 0.5762 - val_loss: 1.0647 - val_acc: 0.6211\n",
            "Epoch 18/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 1.1564 - acc: 0.5870 - val_loss: 1.0479 - val_acc: 0.6250\n",
            "Epoch 19/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 1.1240 - acc: 0.5994 - val_loss: 1.0742 - val_acc: 0.6159\n",
            "Epoch 20/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 1.1064 - acc: 0.6091 - val_loss: 0.9861 - val_acc: 0.6518\n",
            "Epoch 21/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 1.0848 - acc: 0.6143 - val_loss: 1.0053 - val_acc: 0.6372\n",
            "Epoch 22/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 1.0614 - acc: 0.6223 - val_loss: 0.9710 - val_acc: 0.6575\n",
            "Epoch 23/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 1.0451 - acc: 0.6296 - val_loss: 0.9393 - val_acc: 0.6656\n",
            "Epoch 24/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 1.0207 - acc: 0.6410 - val_loss: 0.9029 - val_acc: 0.6825\n",
            "Epoch 25/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 1.0060 - acc: 0.6430 - val_loss: 0.8977 - val_acc: 0.6824\n",
            "Epoch 26/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.9878 - acc: 0.6511 - val_loss: 0.8961 - val_acc: 0.6850\n",
            "Epoch 27/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.9736 - acc: 0.6575 - val_loss: 0.8894 - val_acc: 0.6871\n",
            "Epoch 28/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.9625 - acc: 0.6578 - val_loss: 0.8820 - val_acc: 0.6905\n",
            "Epoch 29/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.9482 - acc: 0.6656 - val_loss: 0.8612 - val_acc: 0.7006\n",
            "Epoch 30/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.9323 - acc: 0.6734 - val_loss: 0.8313 - val_acc: 0.7068\n",
            "Epoch 31/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.9176 - acc: 0.6748 - val_loss: 0.8375 - val_acc: 0.7045\n",
            "Epoch 32/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.9113 - acc: 0.6801 - val_loss: 0.8387 - val_acc: 0.7053\n",
            "Epoch 33/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.8950 - acc: 0.6850 - val_loss: 0.7981 - val_acc: 0.7154\n",
            "Epoch 34/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.8950 - acc: 0.6867 - val_loss: 0.7855 - val_acc: 0.7209\n",
            "Epoch 35/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.8748 - acc: 0.6941 - val_loss: 0.8114 - val_acc: 0.7123\n",
            "Epoch 36/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.8598 - acc: 0.6977 - val_loss: 0.7732 - val_acc: 0.7269\n",
            "Epoch 37/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.8484 - acc: 0.7022 - val_loss: 0.7706 - val_acc: 0.7263\n",
            "Epoch 38/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.8453 - acc: 0.7036 - val_loss: 0.7584 - val_acc: 0.7312\n",
            "Epoch 39/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.8309 - acc: 0.7079 - val_loss: 0.7690 - val_acc: 0.7268\n",
            "Epoch 40/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.8211 - acc: 0.7084 - val_loss: 0.7660 - val_acc: 0.7312\n",
            "Epoch 41/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.8179 - acc: 0.7127 - val_loss: 0.7327 - val_acc: 0.7405\n",
            "Epoch 42/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.8051 - acc: 0.7165 - val_loss: 0.7327 - val_acc: 0.7423\n",
            "Epoch 43/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.7997 - acc: 0.7164 - val_loss: 0.7295 - val_acc: 0.7419\n",
            "Epoch 44/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.7927 - acc: 0.7203 - val_loss: 0.7276 - val_acc: 0.7401\n",
            "Epoch 45/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7849 - acc: 0.7255 - val_loss: 0.7373 - val_acc: 0.7412\n",
            "Epoch 46/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.7745 - acc: 0.7264 - val_loss: 0.7357 - val_acc: 0.7389\n",
            "Epoch 47/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.7684 - acc: 0.7299 - val_loss: 0.7320 - val_acc: 0.7421\n",
            "Epoch 48/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7598 - acc: 0.7329 - val_loss: 0.7145 - val_acc: 0.7510\n",
            "Epoch 49/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.7560 - acc: 0.7366 - val_loss: 0.7199 - val_acc: 0.7486\n",
            "Epoch 50/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.7477 - acc: 0.7387 - val_loss: 0.6973 - val_acc: 0.7521\n",
            "Epoch 51/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.7400 - acc: 0.7416 - val_loss: 0.6747 - val_acc: 0.7631\n",
            "Epoch 52/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.7316 - acc: 0.7403 - val_loss: 0.6964 - val_acc: 0.7546\n",
            "Epoch 53/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.7248 - acc: 0.7423 - val_loss: 0.6886 - val_acc: 0.7577\n",
            "Epoch 54/150\n",
            "30000/30000 [==============================] - 12s 417us/step - loss: 0.7173 - acc: 0.7475 - val_loss: 0.6848 - val_acc: 0.7617\n",
            "Epoch 55/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.7066 - acc: 0.7528 - val_loss: 0.6948 - val_acc: 0.7570\n",
            "Epoch 56/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.7026 - acc: 0.7547 - val_loss: 0.6706 - val_acc: 0.7669\n",
            "Epoch 57/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.6924 - acc: 0.7566 - val_loss: 0.6750 - val_acc: 0.7630\n",
            "Epoch 58/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.6939 - acc: 0.7562 - val_loss: 0.6638 - val_acc: 0.7672\n",
            "Epoch 59/150\n",
            "30000/30000 [==============================] - 12s 409us/step - loss: 0.6852 - acc: 0.7613 - val_loss: 0.6795 - val_acc: 0.7617\n",
            "Epoch 60/150\n",
            "30000/30000 [==============================] - 12s 409us/step - loss: 0.6804 - acc: 0.7620 - val_loss: 0.6684 - val_acc: 0.7634\n",
            "Epoch 61/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6712 - acc: 0.7643 - val_loss: 0.6462 - val_acc: 0.7743\n",
            "Epoch 62/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6713 - acc: 0.7608 - val_loss: 0.6472 - val_acc: 0.7737\n",
            "Epoch 63/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.6630 - acc: 0.7696 - val_loss: 0.6911 - val_acc: 0.7607\n",
            "Epoch 64/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6602 - acc: 0.7694 - val_loss: 0.6649 - val_acc: 0.7670\n",
            "Epoch 65/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.6540 - acc: 0.7697 - val_loss: 0.6413 - val_acc: 0.7747\n",
            "Epoch 66/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.6453 - acc: 0.7716 - val_loss: 0.6410 - val_acc: 0.7780\n",
            "Epoch 67/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.6458 - acc: 0.7727 - val_loss: 0.6324 - val_acc: 0.7774\n",
            "Epoch 68/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6401 - acc: 0.7739 - val_loss: 0.6568 - val_acc: 0.7742\n",
            "Epoch 69/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6298 - acc: 0.7802 - val_loss: 0.6322 - val_acc: 0.7800\n",
            "Epoch 70/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6288 - acc: 0.7773 - val_loss: 0.6407 - val_acc: 0.7772\n",
            "Epoch 71/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.6210 - acc: 0.7813 - val_loss: 0.6391 - val_acc: 0.7778\n",
            "Epoch 72/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.6148 - acc: 0.7846 - val_loss: 0.6691 - val_acc: 0.7703\n",
            "Epoch 73/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.6088 - acc: 0.7843 - val_loss: 0.6193 - val_acc: 0.7866\n",
            "Epoch 74/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 0.6082 - acc: 0.7854 - val_loss: 0.6212 - val_acc: 0.7851\n",
            "Epoch 75/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.6089 - acc: 0.7845 - val_loss: 0.6150 - val_acc: 0.7883\n",
            "Epoch 76/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.5971 - acc: 0.7913 - val_loss: 0.6365 - val_acc: 0.7806\n",
            "Epoch 77/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5844 - acc: 0.7944 - val_loss: 0.6304 - val_acc: 0.7838\n",
            "Epoch 78/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5958 - acc: 0.7920 - val_loss: 0.6283 - val_acc: 0.7850\n",
            "Epoch 79/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.5820 - acc: 0.7951 - val_loss: 0.6181 - val_acc: 0.7860\n",
            "Epoch 80/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.5803 - acc: 0.7969 - val_loss: 0.6223 - val_acc: 0.7827\n",
            "Epoch 81/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5736 - acc: 0.7991 - val_loss: 0.6015 - val_acc: 0.7941\n",
            "Epoch 82/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.5699 - acc: 0.7978 - val_loss: 0.5987 - val_acc: 0.7927\n",
            "Epoch 83/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5660 - acc: 0.7997 - val_loss: 0.6152 - val_acc: 0.7871\n",
            "Epoch 84/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.5618 - acc: 0.7989 - val_loss: 0.6192 - val_acc: 0.7859\n",
            "Epoch 85/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5580 - acc: 0.8017 - val_loss: 0.6163 - val_acc: 0.7909\n",
            "Epoch 86/150\n",
            "30000/30000 [==============================] - 13s 418us/step - loss: 0.5492 - acc: 0.8056 - val_loss: 0.6045 - val_acc: 0.7904\n",
            "Epoch 87/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.5486 - acc: 0.8055 - val_loss: 0.6168 - val_acc: 0.7907\n",
            "Epoch 88/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.5371 - acc: 0.8084 - val_loss: 0.6239 - val_acc: 0.7870\n",
            "Epoch 89/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.5483 - acc: 0.8072 - val_loss: 0.6098 - val_acc: 0.7890\n",
            "Epoch 90/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5401 - acc: 0.8082 - val_loss: 0.6011 - val_acc: 0.7940\n",
            "Epoch 91/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.5335 - acc: 0.8112 - val_loss: 0.6052 - val_acc: 0.7929\n",
            "Epoch 92/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5262 - acc: 0.8145 - val_loss: 0.6137 - val_acc: 0.7914\n",
            "Epoch 93/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.5247 - acc: 0.8137 - val_loss: 0.6182 - val_acc: 0.7887\n",
            "Epoch 94/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.5234 - acc: 0.8163 - val_loss: 0.6200 - val_acc: 0.7906\n",
            "Epoch 95/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.5127 - acc: 0.8179 - val_loss: 0.6099 - val_acc: 0.7945\n",
            "Epoch 96/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.5072 - acc: 0.8197 - val_loss: 0.6111 - val_acc: 0.7945\n",
            "Epoch 97/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.5134 - acc: 0.8190 - val_loss: 0.5882 - val_acc: 0.7998\n",
            "Epoch 98/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.5007 - acc: 0.8216 - val_loss: 0.6077 - val_acc: 0.7953\n",
            "Epoch 99/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.5033 - acc: 0.8221 - val_loss: 0.6023 - val_acc: 0.7974\n",
            "Epoch 100/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.5024 - acc: 0.8206 - val_loss: 0.5827 - val_acc: 0.8024\n",
            "Epoch 101/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4935 - acc: 0.8261 - val_loss: 0.5876 - val_acc: 0.8005\n",
            "Epoch 102/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.4908 - acc: 0.8257 - val_loss: 0.5892 - val_acc: 0.8039\n",
            "Epoch 103/150\n",
            "30000/30000 [==============================] - 13s 420us/step - loss: 0.4897 - acc: 0.8268 - val_loss: 0.5784 - val_acc: 0.8024\n",
            "Epoch 104/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4891 - acc: 0.8251 - val_loss: 0.5947 - val_acc: 0.7981\n",
            "Epoch 105/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4851 - acc: 0.8272 - val_loss: 0.5902 - val_acc: 0.8005\n",
            "Epoch 106/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.4757 - acc: 0.8294 - val_loss: 0.5946 - val_acc: 0.7984\n",
            "Epoch 107/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.4815 - acc: 0.8270 - val_loss: 0.6021 - val_acc: 0.7990\n",
            "Epoch 108/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.4759 - acc: 0.8316 - val_loss: 0.6137 - val_acc: 0.7987\n",
            "Epoch 109/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.4718 - acc: 0.8308 - val_loss: 0.5863 - val_acc: 0.8030\n",
            "Epoch 110/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.4643 - acc: 0.8352 - val_loss: 0.6012 - val_acc: 0.8001\n",
            "Epoch 111/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.4631 - acc: 0.8360 - val_loss: 0.5852 - val_acc: 0.8032\n",
            "Epoch 112/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4606 - acc: 0.8342 - val_loss: 0.5754 - val_acc: 0.8065\n",
            "Epoch 113/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4506 - acc: 0.8396 - val_loss: 0.5779 - val_acc: 0.8047\n",
            "Epoch 114/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4560 - acc: 0.8375 - val_loss: 0.5808 - val_acc: 0.8065\n",
            "Epoch 115/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.4509 - acc: 0.8406 - val_loss: 0.5909 - val_acc: 0.8043\n",
            "Epoch 116/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4426 - acc: 0.8407 - val_loss: 0.5701 - val_acc: 0.8081\n",
            "Epoch 117/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4423 - acc: 0.8431 - val_loss: 0.5878 - val_acc: 0.8042\n",
            "Epoch 118/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4401 - acc: 0.8445 - val_loss: 0.5836 - val_acc: 0.8063\n",
            "Epoch 119/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4307 - acc: 0.8451 - val_loss: 0.5941 - val_acc: 0.8038\n",
            "Epoch 120/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4345 - acc: 0.8452 - val_loss: 0.5755 - val_acc: 0.8073\n",
            "Epoch 121/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4278 - acc: 0.8473 - val_loss: 0.5979 - val_acc: 0.8035\n",
            "Epoch 122/150\n",
            "30000/30000 [==============================] - 12s 406us/step - loss: 0.4279 - acc: 0.8466 - val_loss: 0.5810 - val_acc: 0.8073\n",
            "Epoch 123/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.4211 - acc: 0.8504 - val_loss: 0.5783 - val_acc: 0.8123\n",
            "Epoch 124/150\n",
            "30000/30000 [==============================] - 12s 416us/step - loss: 0.4177 - acc: 0.8502 - val_loss: 0.5875 - val_acc: 0.8066\n",
            "Epoch 125/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4132 - acc: 0.8521 - val_loss: 0.5740 - val_acc: 0.8108\n",
            "Epoch 126/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4109 - acc: 0.8520 - val_loss: 0.5991 - val_acc: 0.8031\n",
            "Epoch 127/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.4130 - acc: 0.8519 - val_loss: 0.5901 - val_acc: 0.8096\n",
            "Epoch 128/150\n",
            "30000/30000 [==============================] - 12s 415us/step - loss: 0.4130 - acc: 0.8494 - val_loss: 0.5854 - val_acc: 0.8098\n",
            "Epoch 129/150\n",
            "30000/30000 [==============================] - 12s 413us/step - loss: 0.4036 - acc: 0.8576 - val_loss: 0.5735 - val_acc: 0.8113\n",
            "Epoch 130/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4092 - acc: 0.8543 - val_loss: 0.5848 - val_acc: 0.8101\n",
            "Epoch 131/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.4045 - acc: 0.8540 - val_loss: 0.5938 - val_acc: 0.8044\n",
            "Epoch 132/150\n",
            "30000/30000 [==============================] - 12s 409us/step - loss: 0.3972 - acc: 0.8581 - val_loss: 0.5747 - val_acc: 0.8138\n",
            "Epoch 133/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.3955 - acc: 0.8565 - val_loss: 0.5785 - val_acc: 0.8105\n",
            "Epoch 134/150\n",
            "30000/30000 [==============================] - 12s 408us/step - loss: 0.3919 - acc: 0.8600 - val_loss: 0.5719 - val_acc: 0.8139\n",
            "Epoch 135/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3906 - acc: 0.8607 - val_loss: 0.5767 - val_acc: 0.8162\n",
            "Epoch 136/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3926 - acc: 0.8591 - val_loss: 0.5943 - val_acc: 0.8095\n",
            "Epoch 137/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3832 - acc: 0.8623 - val_loss: 0.6022 - val_acc: 0.8069\n",
            "Epoch 138/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.3808 - acc: 0.8616 - val_loss: 0.5850 - val_acc: 0.8117\n",
            "Epoch 139/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.3760 - acc: 0.8654 - val_loss: 0.5977 - val_acc: 0.8090\n",
            "Epoch 140/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3757 - acc: 0.8629 - val_loss: 0.5967 - val_acc: 0.8115\n",
            "Epoch 141/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.3738 - acc: 0.8636 - val_loss: 0.5690 - val_acc: 0.8164\n",
            "Epoch 142/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3688 - acc: 0.8682 - val_loss: 0.5802 - val_acc: 0.8144\n",
            "Epoch 143/150\n",
            "30000/30000 [==============================] - 12s 408us/step - loss: 0.3737 - acc: 0.8668 - val_loss: 0.5795 - val_acc: 0.8135\n",
            "Epoch 144/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.3675 - acc: 0.8684 - val_loss: 0.5840 - val_acc: 0.8143\n",
            "Epoch 145/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3672 - acc: 0.8677 - val_loss: 0.5772 - val_acc: 0.8150\n",
            "Epoch 146/150\n",
            "30000/30000 [==============================] - 12s 412us/step - loss: 0.3640 - acc: 0.8690 - val_loss: 0.5854 - val_acc: 0.8163\n",
            "Epoch 147/150\n",
            "30000/30000 [==============================] - 12s 410us/step - loss: 0.3574 - acc: 0.8714 - val_loss: 0.5815 - val_acc: 0.8169\n",
            "Epoch 148/150\n",
            "30000/30000 [==============================] - 12s 414us/step - loss: 0.3639 - acc: 0.8678 - val_loss: 0.5957 - val_acc: 0.8101\n",
            "Epoch 149/150\n",
            "30000/30000 [==============================] - 13s 419us/step - loss: 0.3603 - acc: 0.8679 - val_loss: 0.5904 - val_acc: 0.8136\n",
            "Epoch 150/150\n",
            "30000/30000 [==============================] - 12s 411us/step - loss: 0.3516 - acc: 0.8742 - val_loss: 0.5958 - val_acc: 0.8125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f377db72eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cEI8PwpepSg",
        "colab_type": "text"
      },
      "source": [
        "Gráficas de algunos filtros (canales)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gshIi3rUFRLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "2efe0cd8-9130-4559-d243-66e541d4be95"
      },
      "source": [
        "img1 = model.layers[4].get_weights()[0][:,:,:,:]\n",
        "plt.matshow(img1[0,:,:,5], cmap='viridis')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f377d930128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5wAAAB2CAYAAACkhZyoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCicAAACD+H8GtC5se3zKYQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs6tum1aS_-C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "257b0c9e-b714-44f2-b4bd-70cca349f610"
      },
      "source": [
        "img2 = model.layers[1].get_weights()[0][:,:,:,:]\n",
        "plt.matshow(img1[0,:,:,4], cmap='viridis')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f377d8fda58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5wAAAB2CAYAAACkhZyoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1152x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}